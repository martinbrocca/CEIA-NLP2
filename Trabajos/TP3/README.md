## TP3 - Sistema Multi-Agente RAG con LangGraph

Sistema avanzado de an√°lisis de curr√≠culums usando arquitectura multi-agente con LangGraph. Cada candidato tiene un agente especializado que trabaja colaborativamente para responder preguntas.

**Autor:** Martin Brocca  
**Curso:** CEIA - NLP2  
**Universidad:** Universidad de Buenos Aires

## Descripci√≥n

Sistema RAG multi-agente que utiliza:
- **Router inteligente**: Decide qu√© agentes deben responder cada query
- **Agentes especializados**: Uno por candidato, con retrieval filtrado
- **Confidence scores**: Cada agente reporta su nivel de confianza
- **Agregador**: Sintetiza respuestas de m√∫ltiples agentes

## Video de demostraci√≥n
[Video TP3](./Video/Demo_TP3.mov)
[Video TP3 - Parte2](./Video/Demo_TP3_part2.mov)

## Arquitectura

```mermaid
graph TB
    subgraph "Input"
        A[Pregunta del usuario]
    end
    
    subgraph "Router Node"
        B[Clasificaci√≥n h√≠brida]
        C{Regex match?}
        D[Usar Regex]
        E[Usar LLM]
    end
    
    subgraph "Specialist Agents"
        F[Agent Candidato1<br/>Retriever filtrado]
        G[Agent Candidato2<br/>Retriever filtrado]
    end
    
    subgraph "Aggregator Node"
        H[Sintetizar respuestas<br/>Comparar confidence]
    end
    
    subgraph "Output"
        I[Respuesta + Metadata]
    end
    
    A --> B
    B --> C
    C -->|S√≠| D
    C -->|No| E
    D --> F
    D --> G
    E --> F
    E --> G
    F --> H
    G --> H
    H --> I
    
    style F fill:#90EE90
    style G fill:#87CEEB
    style H fill:#FFD700
```

## Estructura del Proyecto

```
TP3/
‚îú‚îÄ‚îÄ core/                           # M√≥dulos compartidos (de TP2)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                # Exports del paquete
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py              # CleanEmbeddings wrapper
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore.py             # PineconeManager
‚îÇ   ‚îú‚îÄ‚îÄ llm.py                     # LLMFactory (Groq + Anthropic)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Funciones helper
‚îÇ
‚îú‚îÄ‚îÄ state.py                       # AgentState (TypedDict para grafo)
‚îú‚îÄ‚îÄ graph_nodes.py                 # Implementaci√≥n de nodos
‚îÇ   ‚îú‚îÄ‚îÄ RouterNode                 # Routing h√≠brido (regex + LLM)
‚îÇ   ‚îú‚îÄ‚îÄ SpecialistAgentNode        # Agente por candidato
‚îÇ   ‚îî‚îÄ‚îÄ AggregatorNode             # Sintetizador de respuestas
‚îÇ
‚îú‚îÄ‚îÄ multi_agent.py                 # Clase MultiAgentRAG
‚îú‚îÄ‚îÄ chatbot_multi.py               # Interfaz Streamlit
‚îú‚îÄ‚îÄ requirements.txt               # Dependencias (+ langgraph)
‚îú‚îÄ‚îÄ README.md                      # Este archivo
‚îú‚îÄ‚îÄ resumes/                       # Carpeta para PDFs
‚îÇ   ‚îú‚îÄ‚îÄ Candidato1.pdf
‚îÇ   ‚îî‚îÄ‚îÄ Candidato2.pdf
‚îî‚îÄ‚îÄ Video                          # Video de muestra del sistema funcionando
```

## Flujo de Ejecuci√≥n

```mermaid
sequenceDiagram
    participant U as Usuario
    participant R as Router
    participant AM as Agent Candidato1
    participant AA as Agent Candidato2
    participant Agg as Aggregator
    participant UI as Streamlit

    U->>R: "¬øQui√©n tiene m√°s experiencia?"
    
    R->>R: Regex check
    Note over R: No match espec√≠fico
    R->>R: LLM classification
    Note over R: Detecta comparaci√≥n ‚Üí all
    
    par Ejecuci√≥n secuencial
        R->>AM: Ejecutar
        AM->>AM: Retriever (filter: Candidato1)
        AM->>AM: LLM + Confidence
        AM-->>Agg: "Candidato1: 18 a√±os" (conf: 0.85)
    and
        R->>AA: Ejecutar
        AA->>AA: Retriever (filter: Candidato2)
        AA->>AA: LLM + Confidence
        AA-->>Agg: "Candidato2: 15 a√±os" (conf: 0.82)
    end
    
    Agg->>Agg: Sintetizar respuestas
    Agg-->>UI: "Candidato1 tiene m√°s: 18 vs 15 a√±os"
    UI-->>U: Respuesta + Metadata
```

## Instalaci√≥n

### 1. Requisitos previos

- Python 3.11 o superior
- API keys: Pinecone + (Groq o Anthropic)

### 2. Instalar dependencias

```bash
cd Trabajos/TP3
pip install -r requirements.txt
```

**Importante:** TP3 requiere `langgraph>=0.0.40`

### 3. Configurar variables de entorno

Crear `.env` en la ra√≠z:

```bash
PINECONE_API_KEY=tu_api_key
GROQ_API_KEY=tu_api_key          # Opcional
ANTHROPIC_API_KEY=tu_api_key     # Opcional
```

### 4. Preparar carpeta de resumes

```bash
mkdir -p resumes
# Copiar PDFs aqu√≠
```

## Uso

### Ejecutar la aplicaci√≥n

```bash
cd Trabajos/TP3
streamlit run chatbot_multi.py
```

Abre en `http://localhost:8501`

### Flujo de uso

1. **Indexar CVs**
   - Click "Re-indexar CVs"
   - Esperar a que se creen los agentes
   - Verificar agentes en sidebar

2. **Hacer preguntas**
   - Escribir en chat
   - Ver ejecuci√≥n en expander "Detalles"

3. **Interpretar resultados**
   - Respuesta final sintetizada
   - Router reasoning (por qu√© eligi√≥ esos agentes)
   - Confidence scores por agente
   - N√∫mero de documentos recuperados

## Componentes T√©cnicos

### 1. Router Node

**Estrategia h√≠brida:**

```python
# Paso 1: Regex r√°pido
if "martin" in question.lower():
    return ["martin"]

# Paso 2: Si no match, LLM
prompt = "Clasifica: {question}"
result = llm.invoke(prompt)
```

**Casos manejados:**
- Query espec√≠fica ‚Üí Un agente
- Comparaci√≥n ‚Üí Todos los agentes
- General ‚Üí Todos los agentes

### 2. Specialist Agent Node

**Caracter√≠sticas:**
- Retriever filtrado: `filter_dict={"candidate": "Martin"}`
- Top-5 docs por candidato (vs top-10 global en TP2)
- Prompt especializado: "Eres experto en {candidato}"
- Confidence basado en:
  - N√∫mero de docs recuperados
  - Presencia de incertidumbre en respuesta

**C√°lculo de confidence:**

```python
def _calculate_confidence(docs, answer):
    # Base: m√°s docs = mayor confianza
    doc_score = min(len(docs) / 5.0, 1.0)
    
    # Penalizar incertidumbre
    if "no tengo informaci√≥n" in answer.lower():
        return min(doc_score * 0.3, 0.4)
    
    return doc_score
```

**Interpretaci√≥n:**
- üü¢ >= 70%: Alta confianza
- üü° 40-69%: Confianza media
- üî¥ < 40%: Baja confianza

### 3. Aggregator Node

**Comportamiento:**
- **Un agente:** Pass-through (+ disclaimer si conf < 50%)
- **M√∫ltiples:** Sintetiza considerando confidence

**Prompt del agregador:**

```
Combina las respuestas de los agentes:
- Prioriza agentes con mayor confidence
- Haz comparaciones claras
- M√°ximo 2-3 p√°rrafos

Agent Candidato1 (85%): ...
Agent Candidato2 (82%): ...
```

### 4. LangGraph State

**Estado compartido entre nodos:**

```python
class AgentState(TypedDict):
    question: str
    chat_history: list
    selected_agents: list[str]
    router_reasoning: str
    agent_responses: dict
    final_response: str
    metadata: dict
```

**Flujo del estado:**
```
Router ‚Üí selected_agents, router_reasoning
Agents ‚Üí agent_responses (dict de AgentResponse)
Aggregator ‚Üí final_response
```

## Ejemplos de Queries

### Query espec√≠fica

```
Input: "¬øCu√°ntos a√±os de experiencia tiene Candidato1?"

Router: "Regex: Detect√≥ pregunta espec√≠fica sobre martin"
Agents ejecutados: [Candidato1]
Candidato1: 85% confidence, 4 docs
Respuesta: "Candidato1 tiene m√°s de 18 a√±os de experiencia..."
```

### Query comparativa

```
Input: "¬øQui√©n tiene m√°s experiencia en cloud computing?"

Router: "Regex: Detect√≥ comparaci√≥n, consultando todos"
Agents ejecutados: [Candidato1, Candidato2]
Candidato1: 72% confidence, 3 docs ‚Üí "Azure, AWS desde 2015"
Candidato2: 88% confidence, 5 docs ‚Üí "AWS, GCP desde 2017"
Respuesta: "Candidato1 tiene m√°s experiencia (8 a√±os vs 7 a√±os)..."
```

### Query general

```
Input: "¬øAlg√∫n candidato vive en Estados Unidos?"

Router: "LLM: Pregunta general, consultando todos"
Agents ejecutados: [Candidato1, Candidato2]
Candidato1: 95% confidence ‚Üí "S√≠, Texas"
Candidato2: 90% confidence ‚Üí "No, Argentina"
Respuesta: "Candidato1 vive en Estados Unidos (Spring, Texas)..."
```

## Configuraci√≥n Avanzada

### Ajustar k por agente

En `multi_agent.py`:

```python
retriever = self.vectorstore_manager.get_retriever(
    self.embeddings,
    k=5,  # Modificar aqu√≠ (5-10 recomendado)
    filter_dict={"candidate": candidate}
)
```

### Usar modelos diferentes

```python
# Router + Aggregator: Modelo potente
router_llm = LLMFactory.create_llm("claude-sonnet-4-20250514", ...)

# Agents: Modelo r√°pido
agent_llm = LLMFactory.create_llm("llama-3.1-8b-instant", ...)
```

### Agregar nuevo candidato

1. Agregar PDF en `resumes/`
2. Re-indexar
3. Autom√°ticamente se crea Agent_{nuevo_candidato}

## Metadata Disponible

Cada respuesta incluye:

```python
{
    "response": "...",
    "metadata": {
        "router_decision": ["Candidato1", "Candidato2"],
        "router_reasoning": "Regex: Detect√≥ comparaci√≥n...",
        "router_method": "regex",  # o "llm"
        "agents_executed": 2,
        "agent_details": {
            "Candidato1": {
                "confidence": 0.85,
                "retrieved_docs": 4
            },
            "Candidato2": {
                "confidence": 0.82,
                "retrieved_docs": 5
            }
        }
    }
}
```

## Troubleshooting

### Error: "module 'langgraph' has no attribute 'graph'"

```bash
pip install --upgrade langgraph
```

### Confidence siempre baja

- Verificar que los PDFs tengan contenido relevante
- Aumentar k en retrievers
- Revisar filtros de metadata

### Router siempre usa LLM

- Los nombres de candidatos no est√°n en la query
- Ajustar regex patterns en `RouterNode._regex_route()`

### Agentes no se ejecutan en paralelo

- Esto es intencional (ejecuci√≥n secuencial)
- Para paralelizaci√≥n: modificar `_run_agents_node()` con asyncio

## Extensiones Futuras

### 1. Paralelizaci√≥n con asyncio

```python
async def _run_agents_node_async(state):
    tasks = [agent(state) for agent in selected_agents]
    responses = await asyncio.gather(*tasks)
    return responses
```

### 2. Cache de respuestas

```python
cache = {}
cache_key = f"{question}_{agent_name}"
if cache_key in cache:
    return cache[cache_key]
```

### 3. Agente de fallback

Si todos los agentes tienen confidence < 50%:
```python
if max(confidences) < 0.5:
    return "No tengo suficiente informaci√≥n para responder con confianza"
```

### 4. M√©tricas de timing

```python
import time

start = time.time()
# ... ejecutar nodo ...
metadata["timing"] = {
    "router": 0.2,
    "agents": 1.3,
    "aggregator": 0.5
}
```


## Contacto

Martin Brocca - CEIA, Universidad de Buenos Aires  
martinbrocca@gmail.com
